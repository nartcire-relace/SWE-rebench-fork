# This file contains logic for running evaluations on TractoAI: <https://tracto.ai/>.

import yt.wrapper as yt
import yt.type_info as ti
from typing import Iterable
import datetime
import dataclasses
import json
from yt import yson

from pathlib import Path
from swebench.harness.reporting import make_run_report
from typing import cast, Annotated


from swebench.harness.constants import (
    RUN_EVALUATION_LOG_DIR,
    SWEbenchInstance,
)
from swebench.harness.test_spec.test_spec import make_test_spec, TestSpec


yt.config["pickling"]["ignore_system_modules"] = True
yt.config["pickling"]["dynamic_libraries"]["enable_auto_collection"] = False


@yt.yt_dataclass
class TestInput:
    test_spec: Annotated[
        bytes,
        yt.schema.types.Annotation(
            ti_type=ti.Yson,
            to_yt_type=lambda x: yson.dumps(dataclasses.asdict(x)),
            from_yt_type=lambda x: TestSpec(**yson.loads(x)),
        )
    ] | None
    prediction: Annotated[
        bytes,
        yt.schema.types.Annotation(
            ti_type=ti.Yson,
            to_yt_type=lambda x: yson.dumps(x),
            from_yt_type=lambda x: yson.loads(x)
        )
    ] | None
    run_id: str
    timeout: int


@yt.yt_dataclass
class TestOutput:
    instance_id: str
    test_output: str
    report_json_str: str
    run_instance_log: str
    patch_diff: str
    log_dir: str
    errored: bool


def get_log_dir(pred: dict, run_id: str, instance_id: str) -> Path:
    model_name_or_path = cast(
        str, pred.get("model_name_or_path", "None").replace("/", "__")
    )
    return RUN_EVALUATION_LOG_DIR / run_id / model_name_or_path / instance_id


class RunInstanceTracto(yt.TypedJob):
    def __call__(self, test_input: TestInput) -> Iterable[TestOutput]:
        test_spec = cast(TestSpec, test_input.test_spec)
        prediction = cast(dict, test_input.prediction)
    
        yield TestOutput(
            instance_id=test_spec.instance_id,
            test_output="test_output_dummy",
            report_json_str=json.dumps({
                test_spec.instance_id: {"resolved": False},
            }),
            run_instance_log="dummy_run_instance_log",
            patch_diff="dummy_patch_diff",
            log_dir=str(
                get_log_dir(prediction, test_input.run_id, test_spec.instance_id)
            ),
            errored=False
        )


def run_instances_tracto(
    predictions: dict[str, dict],
    instances: list[SWEbenchInstance],
    full_dataset: list[SWEbenchInstance],
    run_id: str,
    timeout: int,
):
    """
    Run all instances for the given predictions on Modal.

    Args:
        predictions (dict): Predictions dict generated by the model
        instances (list): List of instances
        run_id (str): Run ID
        timeout (int): Timeout for running tests
    """
    test_specs = [make_test_spec(instance) for instance in instances]

    run_test_specs: list[TestSpec] = []

    # Check for instances that have already been run
    for test_spec in test_specs:
        log_dir = get_log_dir(
            predictions[test_spec.instance_id], run_id, test_spec.instance_id
        )
        if log_dir.exists():
            continue
        run_test_specs.append(test_spec)

    if run_test_specs:
        run_dir = (
            "//home/llm/sbkarasik/tracto-swe-bench/"
            f"{run_id}-{datetime.datetime.now().isoformat()}"
        )
        print(f"{run_dir=}")

        yt.create("map_node", run_dir, recursive=True)

        input_table_path = f"{run_dir}/input"
        output_table_path = f"{run_dir}/output"
        print(f"{input_table_path=}")

        source_table_rows = [
            TestInput(
                test_spec=test_spec,
                prediction=predictions[test_spec.instance_id],
                run_id=run_id,
                timeout=timeout,
            )
            for test_spec in run_test_specs
        ]
        yt.write_table_structured(
            table=input_table_path,
            row_type=TestInput,
            input_stream=source_table_rows,
        )

        yt.run_map(
            RunInstanceTracto(),
            input_table_path,
            output_table_path,
            spec={
                "mapper": {
                    "docker_image": "cr.turing.yt.nebius.yt/home/llm/sbkarasik/registry/swebench-fork:dev",
                },
                "max_failed_job_count": 1,
            }
        )
        
        for result in yt.read_table_structured(output_table_path, TestOutput):
            result = cast(TestOutput, result)

            # Save logs locally
            log_dir = Path(result.log_dir)
            log_dir.mkdir(parents=True, exist_ok=True)
            with open(log_dir / "run_instance.log", "w") as f:
                f.write(result.run_instance_log)
            with open(log_dir / "test_output.txt", "w") as f:
                f.write(result.test_output)
            with open(log_dir / "patch.diff", "w") as f:
                f.write(result.patch_diff)
            with open(log_dir / "report.json", "w") as f:
                try:
                    report_json = json.loads(result.report_json_str)
                    json.dump(report_json, f, indent=4)
                except Exception:
                    # This happens if the test fails with any exception
                    print(f"{result.instance_id}: no report.json")

    make_run_report(predictions, full_dataset, run_id)
